---
layout: default
title: 2022/12/06 golang基础2 【学习笔记】
author: sindweller <sindweller5530@gmail.com>
tags: [编程语言]
---
## 线程安全：
多核架构中，不同线程可能跑在不同cpu上。  
CPU读自己的缓存数据比读取在物理内存中的数据快  
第一次读取是从物理内存读取，之后会缓存到各个cpu的cache里。如果cpu1上的线程先修改到本地缓存但没有写到内存，cpu2上的线程读取到的还是缓存里的原值，cpu上的缓存可能没有同步。引起线程安全问题  

解决方案：
### 加锁
多个线程访问同一块共享内存，要加锁，锁中需要写数据，会同步回主内存里。这个锁是一个互斥锁，加锁时其他线程无法访问这个内存。
### Go
go的chan隐藏了锁的原语的复杂性，但是如果非要共享内存的方式来通信，也是可以支持的。 使用Sync包，其中包括：

- sync.Mutext 互斥锁 包含Lock() Unlock()
- sync.RWMutext 读写分离锁，多读唯一写
- sync.WaitGroup 等待一组goroutine返回
- sync.Once 确保一段代码只执行一次
- sync.Cond 一组goroutine再特定条件下才被唤醒 在k8s生产者消费者模型中会用到。

k8s informer机制 监听k8s对象
informer是map，map本身不能并发读写，发生冲突时会panic。
因此在修改map时要加锁

```yaml
for i, pod := range pods{
	wg.Add(1)
	go func(){defer wg.Done()}()
}
wg.Wait() // 等待循环里的所有go执行完成
```
condition: 处理边界问题
```yaml
func (q *Queue) Dequeue() string {
	// cond *sync.Cond
	// cond: sync.NewCond(&sync.Mutex{}) 也就是q.cond.L
	// enqueue往队列里加入数据的时候要先加锁 q.cond.L.Lock()
	q.cond.L.Lock() //读取的时候也要加锁
	defer q.cond.L.Unlock()
	if len(q.queue) == 0{
  	fmt.Println("wait")
  	//当队列长度为0就要调用cond的wait告知其稍等一下 
		//等到生产者生产数据之后，在生产者这边告知：q.cond.Broadcast()
  	//然后消费者阻塞的线程会得知消息，继续接收
  	q.cond.Wait()
	}
	result := q.queue[0]
	q.queue = q.queue[1:] // 出队
}
```
## 内存
linux把内存分页，每个是4k大小。对内存管理实际上是对页进行管理。可以使用 `getconf PAGE_SIZE`命令获取。
每个进程在程序里看到的是虚拟地址，虚拟地址和物理地址有映射关系。
如何利用虚拟地址？
64位和32位系统的内存地址的空间大小不一样。最大支持是4个G。
虚拟地址构成：

- kernel space 1G内存 预留
- 参数环境变量
- 堆 低地址区
- 未分配内存
- 栈 高地址区
- BSS 未初始化数据 （变量）
- Data 初始化数据 （变量）
- Text 程序代码

未分配内存在stack与heap之间。
找到一个编译好的二进制go程序文件，可以使用size myproject来查看其虚拟地址是如何使用的（mac的输出跟这个不太一样）。
```yaml
[root@curveadm sbin]# size curve
   text	   data	    bss	    dec	    hex	filename
14649551	 839384	 252368	15741303	 f03177	curve
```
通过 `objdump -x myproject`来查看二进制文件里面的详细内容
```shell
0000000000a8ed20 g     F .text	0000000000000315              github.com/opencurve/curve/tools-v2/pkg/cli/command/curvefs/warmup/add.(*AddCommand).RunCommand
```
go编译器会往文件里面放

- 很多初始化变量和未初始化变量
- runtime相关的代码
```shell
00000000011fbd38 g     O .noptrdata	0000000000000008              runtime.intArgRegs
00000000012cb0e0 g     O .bss	0000000000000018              runtime.pinnedTypemaps
0000000001210f80 g     O .noptrdata	0000000000000228              runtime.firstmoduledata
00000000011fbd40 g     O .noptrdata	0000000000000008              runtime.lastmoduledatap
00000000012c7e38 g     O .bss	0000000000000008              runtime.modulesSlice
00000000012fc560 g     O .noptrbss	0000000000000008              runtime.faketime
00000000012c7e40 g     O .bss	0000000000000008              runtime.overrideWrite
00000000012d56c0 g     O .bss	00000000000100f8              runtime.trace
```
### 页表项
虚拟地址和物理地址之间的映射关系。

- 一对一映射，占内存过大

4级页表：（类似索引）从上往下查

- PGD page global directory
- PUD page upper directory
- PMD page middle directory
- PT page table

有效缩减了索引的大小。
## 页缺失
- minflt 次页缺失：当虚拟内存真正需要被用到的时候，就会转换成物理内存。如果一次被用到时，物理内存中其实没有分配，就会产生一个次页缺失，一般问题不大。
- majflt 主页缺失：如果请求的数据不在物理内存中，要从磁盘或交换分区（swap）换到内存中，就是主页缺失。非常影响性能。
- OOM
  - 次页缺失时，恰好系统无法释放出内存，就会产生OOM。对于kernel 2.6.x来说会先从占用内存最多的进程开始杀死进程，直到内存够用。kernel 4.x 会进行评分，然后杀死进程。
### 查看页缺失
```go
root@dev:~# ps o pid,comm,minflt,majflt
824    PID COMMAND         MINFLT MAJFLT    824 dockerd          11953    320
```
## 物理内存超用 Overcommit
因为有虚拟内存，所以会存在物理内存超用现象。kernel中用一个参数来控制overcommit来尝试准确分配内存。
## CPU对内存的访问
CPU上有一个MMU（Memory Management Unit）单元
CPU把虚拟地址给MMU， MMU去物理内存查询页表得到实际的物理地址
CPU缓存TLB（Translation Lookaside Buffer），虚拟地址和物理地址的映射关系
这样MMU不必每次都去查页表
### 进程切换产生的开销

- 直接开销
   - 切换页表全局目录PGD
   - 切换内核态堆栈
   - 切换硬件上下文（必须装入寄存器的数据）
   - 刷新TLB（失去对映射的缓存）
   - 系统调度器代码执行
- 间接开销
   - 增加了CPU缓存失效导致进程需要到内存直接访问的IO操作
### 线程切换开销

- 一组线程因为共享内存资源，所以共享虚拟地址空间，线程切换相比进程切换节省了虚拟地址空间的切换。
- 绕过系统调用而完成的线程切换方式：用户线程
#### 用户线程
完全在用户态执行，app在用户空间创建的可执行单元。多个轻量级线程绑定到同一个kernel thread。
在时间片分到kernel thread时执行完所有用户线程
#### goroutine
基于GMP模型

- M kernel层面的一个task，也就是内核线程，记录内核线程栈信息，如果g调度到m上，使用g自己的栈信息
- P 绑定G和M，维护用户态的运行队列，是一个调度器，此外还负责一些内存管理
- G 协程，每个g都有自己的栈空间和定时器

内核态：

- KSE kernel scheduling entity

用户态：

- GMP

不在运行队列中的g

- channel阻塞态的G位于sudo G
- 脱离P绑定在M上的G 如系统调用
- 为了复用，执行结束进入P的gFree列表中的G
### goroutine创建过程

- 复用或创建新的goroutine结构体
   - gFree列表里查找空闲goroutine
   - 通过runtime.malg 创建一个栈大小足够的新结构体（malloc）
- 函数传入的参数转移到goroutine栈上
- 更新goroutine与调度相关的属性，更新其状态未_Grunnable
- 返回的goroutine会存储到全局变量allgs中
### goroutine放到运行队列上

- goroutine设置到处理器的runnext作为下一个处理器执行的任务
- 本地转全局：当处理器的本地运行队列没有剩余空间（256满）时，就会把本地队列中的一部分goroutine和待加入的goroutine通过runtime.runqputslow添加到调度器持有的全局运行队列上。
### 调度器行为

- 保证公平，全局运行队列有待执行g时，通过schedtick保证有一定几率（1/61）从全局队列中查找对应g
- 从处理器本地队列中查找待执行的g
- 如果前面都没找到，会通过runtim.findrunnable 阻塞地找g
   - 本地、全局找
   - 网络轮询器中找是否有g等待运行
   - 通过runtime.runqsteal从其他随机处理器中偷一半待运行的g

防止全局队列里的g被饿死

## 内存管理
堆内存管理

- Mutator 用户程序，需要通过下面的Allocator创建对象
- Allocator 内存分配器，动态切堆内存分配给用户程序 并在下面的Header中记录哪块内存被使用了。已分配内存会记录为链表。
- Object Header 对象头，供下面的Collector和上面的Allocator同步对象元数据（描述内存块的元数据：size used next下一个内存块的地址 data)。内存的分配和回收实际上就是对这个头做修改。
- Collector 垃圾回收器，回收内存空间，会扫描Heap里的对象头，看哪些是非活跃对象，回收这些内存
### TCMalloc Go的内存管理起源
面向虚拟内存，如果每次申请内存都需要malloc发起系统调用，效率比较低，就需要预分配。
多个线程同时申请内存要加锁。

- 每个线程维护本地的ThreadCache，是每个线程独立的内存空间，优先从这个独占的cache中申请空间，如果用完了，会向CentralCache申请空间，这个时候就要加锁了。优化是减少向CentralCache申请的次数，每次多申请一些，
- 如果CentralCache没有空间了，会向上到PageHeap进行申请
- PageHeap没有空间会向VirtualMemory申请

如果申请的内存空间是无序的，会导致链表里的各个分配内存大小不一。为此，TCMalloc做的调整优化是：将整个内存分为不同的等级（Size class）。
每申请一页（8k，与linux的内存页不同），会把申请的内存按照size class划分（总共128个），例如按8B、16B分。相同大小会组成一个span list。这样在申请的时候先找属于哪个区间（size class），再从对应的size class里分配内存片段。
### 基本概念

- page 内存页 8K Go与操作系统之间的内存申请和释放都是以page为单位
- span 内存块 由多个连续page组成
- size class 空间规格 每个span都配有一个sizeclass 表明这个span中的page应当如何使用（如何切分，切分多大）
- object 对象 存储变量数据内存空间。span在初始化时会按照size class切分为多个相等的object 需要用的时候就分配一个object出去
- 根据对象大小不同，申请的地方也不一样
   - 小对象 优先ThreadCache 逐级向上
   - 中对象 直接PageHeap中选择适当大小，128Page的Span保存的最大内存就是1MB
   - 大对象 从large span set选择合适的多个页组成span
## Go的内存分配
go魔改了TCMalloc，层级基本一样。

- 两个大小一样的spanclass对应一个size class，其中一个存指针，一个存直接引用。直接引用的span不需要gc
- mheap里不是链表而是排序二叉树，内存初始化在二叉树里，更快搜索。一棵树叫free，是刚申请的。另一棵是scav，是gc回来的内存。
## 内存回收

- 引用计数（python php switft）
   - 对象可以很快回收
   - 循环引用怎么办？
- 标记-清楚（go)
   - 从根对象开始扫描所有引用的对象，引用的对象标记为“被引用”，没有被标记的回收
   - 需要STW 要暂停程序运行（1.20是否有改变？）
- 分代收集（java）
   - 按生命周期分频率回收
## mspan（gc原理）

- allocBits 记录每块内存的分配情况
- gcmarkBits 记录每块内存的引用情况

位图可以很方便的标记哪个内存有没有被引用。有引用的对象标记为1
回收时，将allocBits覆盖到gcmarkBits，未进行标记（0）就回收
## GC工作流程
大部分处理是和用户代码并行

- mark
   - mark prepare：初始化GC任务 在此期间会stw 停止用户态程序
      - 开启写屏障 （write barrier）
      - 辅助 gc （mutator assist）
      - 统计root对象的任务数量
   - GC Drains： 扫描所有root对象（这个过程中用户代码已经恢复运行了） 包括全局指针和G上的指针（扫描到对应G栈时需要停止G），将其加入标记队列（灰色队列），循环处理灰色队列的对象直到为空。这个过程是后台执行的。因为在整个过程中用户代码还在跑，可能修改root对象，因此还需要下面的re-scan操作。
- Mark Termnation： 完成标记工作，重新扫描re-scan全局指针和栈。但在此过程中也可能会有新对象分配和指针赋值，需要写屏障记录下来，re-scan再检查一下，这个过车也会STW。
- Sweep： 按照标记结果回收所有白色对象（经过灰色队列标记处理的是黑色，没有被标记的是白色），后台并行
- Sweep Termination: 对未清扫的span进行清扫，上一轮gc的清扫完成后才能开启新的一轮gc

![golang-gc](/assets/golanggc.jpeg)

相比java优化了STW，且使用户基本对gc无感。
## 三色标记逻辑

- GC开始时，所有object都是白色，就全是垃圾
- 从root区开始遍历，遍历到的object置为灰色
- gc drains：遍历所有灰色，将内部引用变量置为灰色，自身置为黑色
- 循环上一步，直到队列里没有灰色object，只剩下黑白，白色的都是垃圾
- 对于黑色obj，如果标记期间发生了写操作，写屏障会在真正赋值前将新对象标记成灰色，避免视为白色
- 标记过程中，mallocgc新分配的object，会先标记成黑色再返回，避免视为白色
## 垃圾回收触发机制

- 没内存了——内存分配量达到阈值（上次GC内存分配量*内存增长率（GOGC控制，默认100，即内存扩大一倍时启用））
- 到时间了——默认最长2min触发一次gc，时间间隔在src/runtime/proc.go:forcegcperiod中声明
- 手动触发——runtime.GC()
## 查看gc情况
```shell
GODEBUG=gctrace=1 go run main.go
gc1 @0.083s 0%: 0.046+1.9+0.002 ms clock, 0.37+1.7/0.88/2.5+0.019 ms cpu, 4->4->0 MB, 5 MB goal, 8 P
gc 2 @0.170s 0%: 0.14+0.42+0.038 ms clock, 1.1+0/0.58/0.73+0.30 ms cpu, 4->4->0 MB, 5 MB goal, 8 P
gc 3 @0.190s 0%: 0.12+0.41+0.003 ms clock, 0.97+0.35/0.30/0.89+0.026 ms cpu, 4->4->0 MB, 5 MB goal, 8 P
```